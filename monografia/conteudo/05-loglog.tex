\chapter{\textbf{LogLog's}}

Em 1978, Morris resolveu o problema de estimar quantos elementos passaram por um fluxo de dados sem armazenar esse valor 
em um contador. E as ideias principais da \hyperref[chap:morris:algorithm]{solução dele} era guardar uma aproximação do 
logaritmo dessa quantidade e incrementá-la por meio de um método probabilístico que lembra lançamentos de moedas. Dessa 
forma, se temos um fluxo com $n$ elementos, e $X$ é a aproximação do algoritmo de Morris para~$\lg n$, então $X$ deve 
ser incrementado com probabilidade $1 \mathbin{/} 2^X$ para cada novo elemento, e isto é equivalente a lançarmos uma 
moeda $X$ vezes e aumentar $X$ somente se todas as jogadas forem cara. 

Essa solução inspirou o desenvolvimento do algoritmo da $\pcounting$, que resolvia o problema de estimar a quantidade de 
elementos distintos que passaram por um fluxo~$\Mbb$. Nesse \hyperref[sec:flajolet-martin:algorithm]{algoritmo}, estamos 
interessados em usar \hyperref[sec:flajolet-martin:pattern]{padrões de bits} de números inteiros aleatórios para gerar 
essa estimativa. Portanto, os dados de $\Mbb$ são mapeados para inteiros por meio de uma função de hash, e assim, 
podemos considerar que esses elementos são na verdade palavras binárias de tamanho infinito. Essa simplificação nos 
permite agrupar os itens de $\Mbb$ por prefixos da forma $0^{*}1$. A razão de fazermos isto é que a probabilidade de a 
representação binária de um inteiro ter um prefixo da forma $0^{X-1}1$ é $1 \mathbin{/} 2^{X}$, a mesma que aparece no 
algoritmo de Morris, e portanto, essa ideia pode ser usada para estimarmos o logaritmo da quantidade de elementos 
distintos em $\Mbb$ da seguinte forma: guardaremos o menor $R$ tal que ainda não tenha aparecido algum inteiro com 
prefixo~$0^{R}1$.

Dessa maneira, para um fluxo $\Mbb$ com $n$ elementos distintos, $R$ seria a estimativa para~$\lg n$. No entanto, por 
meio do cálculo do valor esperado desse estimador, é possível concluir que $R$, na verdade, não estima $\lg n$, mas 
$\lg \phi n$~\citep{flajolet:martin:85}. Em outras palavras, o algoritmo da $\pcounting$ produz uma estimativa com um 
desvio $\phi$ mensurável e podemos, em vista disso, corrigi-lo.

Contudo, mesmo corrigindo esse erro do estimador, este ainda possui uma grande variância, ou seja, a estimativa 
devolvida pode estar muito próxima ou muito longe do valor real. Para contornar essa situação, podemos repetir várias 
vezes o algoritmo da $\pcounting$ e encontrar a média dos estimadores. E uma forma interessante de se fazer isso em uma 
única iteração, removendo assim, a necessidade de executarmos o algoritmo muitas vezes, é dividir de modo uniforme os 
dados do fluxo em $m$ lotes. Então, um inteiro $x$ faria parte do lote $x \bmod m$, e olharíamos para o prefixo de 
$\lfloor x \mathbin{/} m \rfloor$. Desse modo, passaríamos a guardar um estimador $R$ para cada lote, e a estimativa 
para $n$ usaria a média desses valores. Essa técnica é conhecida como \textbf{média estocástica}.

Por fim, o algoritmo da $\pcounting$ tem consumo proporcional a $O(mL)$ bits, em que $m$ é o número de lotes e $L$ é a
quantidade de bits necessária para armazenar algum inteiro do fluxo. Esse consumo é decorrente do fato de cada lote 
precisar guardar a informação se um prefixo da forma $0^{*}1$ já apareceu, e podemos fazer isto com $L$ bits por lote. 
Nesse sentido, o próximo algoritmo, chamado $\LOG$, terá como base muitas ideias da $\pcounting$, e apresentará uma 
significativa redução do consumo de espaço.

\section{Algoritmo $\LOG$}
\label{sec:loglog:algorithm}

Dado um fluxo $\Mbb$ com $n$ elementos distintos, o algoritmo~$\LOG$ devolverá um estimador~$\hat{n}$ para $n$, que terá
um desvio padrão de $1{,}30 \mathbin{/} \sqrt{m}$~\citep{loglog:03}. Assim como foi feito no algoritmo da $\pcounting$, 
precisamos ter uma função de hash~$h$ que mapeie uniformemente os dados de um fluxo para inteiros. Tendo esta função em 
mãos, podemos supor que estamos trabalhando com um fluxo de inteiros aleatórios. E vamos manter o maior~$M$ tal que 
exista algum elemento em~$\Mbb$ cuja representação binária tenha prefixo da forma $0^{M-1}1$.

Para $M \geq 1$, a probabilidade de um inteiro ter um prefixo da forma~$0^{M-1}1$ é $1 \mathbin{/} 2^{M}$, ou seja, 
esperamos que um em cada $2^{M}$ elementos tenha esse prefixo. Por outro lado, podemos inverter essa ideia, e pensar que 
se temos um inteiro com prefixo da forma $0^{M-1}1$, então pelo menos $2^{M}$ elementos distintos devem ter aparecido. 
Desse modo, o valor de $M$ será uma aproximação para~$\lg n$, e podemos interpretá-lo como sendo a posição indexada a 
partir do 1 do bit ligado menos significativo de um inteiro. Uma ideia similar foi vista na 
Seção~\ref{sec:flajolet-martin:algorithm}, em que definimos a função $\rho$ que retorna a posição indexada a partir 0 do 
bit ligado menos significativo de um número. Em vista disso, vamos definir a função $\rho_1$ que retorna o valor 
anterior só que indexado a partir do 1, e utilizá-la para o cálculo de~$M$.

Para reduzirmos a variância da estimativa, dividimos o fluxo~$\Mbb$ em $m = 2^{k}$ lotes de acordo com os bits dos 
elementos. Dessa forma, um inteiro $x = \langle b_1 b_2 {\dots} b_k {\dots} \rangle$ fará parte do lote 
$y = \langle b_1 b_2 {\dots} b_k \rangle$, ou seja, os $k$ primeiros bits de um elemento indicam em qual lote ele 
pertence. Em seguida, encontramos $M_y$ tal que $0^{M_y}1$ é prefixo de $\langle b_{k+1} b_{k+2} {\dots} \rangle$, e 
mantemos o maior valor de $M_y$.

Desse modo, o valor de $M_y$ de cada lote~$y$ será uma estimativa para $\lg \frac{n}{m}$. Para diminuirmos a variância 
dessa aproximação, calculamos a média \textbf{aritmética} $\overline{M} = \frac{\sum_y M_y}{m}$. Logo, $\overline{M}$ 
aproxima $\lg \frac{n}{m}$, e consequentemente, $2^{\overline{M}}$ aproxima $\frac{n}{m}$. E, portanto, a estimativa 
para $n$ será $m \times 2^{\overline{M}}$.

No entanto, essa estimativa apresenta um desvio e para corrigí-lo, multiplicamos ela por uma constante $\alpha_m$ cujo 
valor é proporcional ao número de lotes utilizados no algoritmo. Na prática, podemos usar como fator de correção 
$\alpha_\infty \approx 0{,}39701$ assim que $m$ for maior que~64. Em vista disso, a saída do algoritmo~$\LOG$ será um 
estimador $\hat{n}$ da forma $\alpha_m \times m \times 2^{\overline{M}}$. E o pseudocódigo a seguir condensa as ideias 
apresentadas.

\begin{codebox}
  \Procname{$\logprogram\big(\Mbb, m = 2^k\big)$}
  \li \For $i$ de $0$ até $m$
        \Do
  \li   $M[i] \gets 0$
        \End
  \li \For cada $x$ em $\Mbb$ 
        \Do
  \li   $b_1b_2{\dots} \gets h(x)$
  \li   $lote \gets \langle b_1 {\dots} b_k \rangle$
  \li   $M[lote] \gets \max(M[lote], \rho_1(b_{k+1}b_{k+2}{\dots}))$
        \End
  \li
  \Return $\alpha_m \times m \times 2^{\frac{1}{m}\sum_i{M[i]}}$   
  \End
\end{codebox}

Vamos simular um exemplo pequeno para entendermos com mais detalhes o algoritmo~$\logprogram$. O objetivo, portanto, é 
estimar a quantidade de elementos distintos em $\Mbb = \{ 50, 85, 45, 29, 89, 82, 87, 10, 92 \}$. Suponhamos que 
$k = 2$, $m = 2^{k} = 2^2 = 4$, que a função de hash~$h$ seja a função identidade e que $\alpha_m = \alpha_4 = 1$, 
ou seja, estamos desconsiderando o desvio da estimativa devolvida pelo algoritmo. 

Inicialmente, os vetores $M$ estão zerados, ou seja, $M[i] = 0$ para $0 \leq i < 4$. Na primeira iteração do algoritmo,
temos que verificar em qual lote o elemento $50 = 010011_2$ se encontra. Como $k = 2$, os primeiros $2$ bits definem o
lote e assim, $50$ faz parte do lote $01_2 = 2$. Ao final da primeira iteração, temos que $M[2] = 3$, já que 
$\rho_1(0011_2) = 3$ e dessa forma, $0011_2$ tem prefixo da forma $0^21$

Na segunda iteração, o número do lote de $85 = 1010101_2$ é $1$, e $\rho_1(10101_2) = 1$. Desse modo, $M[1] = 1$. Os 
próximos elementos a serem considerados, $45 = 101101_2$ e $29 = 10111_2$, fazem parte do lote de número~$1$, e têm 
prefixo da forma $0^01$. Dessa maneira, $M[1]$ continua sendo um após esses itens serem processados. Precisamos, assim,
analisar $89 = 1001101_2$, que pertence ao lote~$1$. Como $\rho_1(01101_2) = 2$, $M[1]$ passa a ter valor $2$.

O próximo elemento de $\Mbb$ é $82 = 0100101_2$ cujo lote é $2$. Temos que $\rho_1(00101) = 3$, mas o valor de $M$ do 
lote~$2$ já é $3$, sendo assim, $M[2]$ continua com o mesmo valor. Continuando o exemplo, devemos processar 
$87 = 1110101_2$. Este item está no lote~$3$, e $\rho_1(10101_2) = 1$. Logo, $M[3]$ passa a ser igual a 1. No caso 
seguinte, $10 = 0101_2$ faz parte do lote $2$ e $\rho_1(01_2) = 2$. Contudo, $M[2]$ é maior que $2$ e portanto, não é 
atualizado. Por fim, $92 = 0011101_2$ vai para o lote~$0$, $\rho_1(11101_2) = 1$ e $M[0] = 1$.

Os valores de $M$ ao processarmos todos os elementos de $\Mbb$ são: $M[0] = 1$, $M[1] = 2$, $M[2] = 3$ e $M[3] = 1$. 
Logo, o valor médio $\overline{M}$ de $M$ é $(1 + 2 + 3 + 1) / 4 = 7 / 4 = 1{,}75$. E a estimativa para a quantidade de 
itens distintos em 
$\Mbb$ é $\alpha_m \times m \times 2^{\overline{M}} = 1 \times 4 \times 2^{1{,}75} = 13{,}45{\dots} \approx 13$.

\section{Consumo de espaço do $\LOG$}

No algoritmo da $\pcounting$, vimos a ideia de dividirmos os elementos de um fluxo de dados em $m$ lotes. E como cada 
lote armazena um vetor $\bitmap$ com $L$ bits, o consumo de espaço dessa solução é de $O(mL)$ bits. Assim, para 
$m = 1024$ e $L = 32$, a $\pcounting$ gasta $4$ KB de memória para estimar o número de itens distintos em um fluxo.

Por outro lado, o algoritmo~$\LOG$ também divide os dados de um fluxo $\Mbb$ com $n$ elementos distintos em $m$ lotes. 
Só que ao invés de um lote guardar a informação se um prefixo já apareceu ou não, ele armazena uma aproximação para 
$\lg n$. Logo, se para representarmos um inteiro~$x$, precisamos de pelo menos $\lg x$ bits, para guardar $\lg n$, são
necessários $\lg \lg n$ bits. Este fato deu origem ao nome do algoritmo.

Considerando o exemplo anterior em que $m = 1024 = 2^{10}$ e que os dados do fluxo são inteiros de $32$ bits, os 10 
primeiros bits definem o lote de um elemento e os 22 restantes serão utilizados para encontrar $M$. Dessa forma, temos 
que $M \leq 22$ e precisamos de somente $5$ bits para guardar esse valor. Portanto, o custo de espaço do 
algoritmo~$\LOG$ nesse caso é de $0{,}625$~KB, que é cerca de seis vezes menor que na $\pcounting$.

Essa diferença é maior ainda se trabalharmos com inteiros de $64$ bits e valores de $m$ na ordem de $65536 = 2^{16}$
para termos estimativas mais precisas. No algoritmo~$\LOG$, $M$ pode ser armazenado com 6 bits, pois $M \leq 48$, e 
desse modo, o custo de memória é de $50$~KB, dez vezes menos que na $\pcounting$, que gasta aproximadamente 
$524$~KB.

\section{Implementando $\LOG$}

\begin{lstlisting}[style=mypython,caption=Implementação do algoritmo $\logprogram$,captionpos=b, label=loglog:code]
class LogLog:
    def __init__(self, m=64):
        self.m = m
        self.B = math.floor(math.log2(m))
        self.M = [0] * m
        self.Z = 0
        self.alpha = 0.39701
  
    def p(self, x: int):
        return (x & -x).bit_length()

    @property
    def prefixo(self):
        return (1 << self.B) - 1

    def adiciona(self, x: int):
        lote = x & self.prefixo
        w = x >> self.B

        if self.p(w) > self.M[lote]:
            self.Z -= self.M[lote]
            self.M[lote] = self.p(w)
            self.Z += self.M[lote]

    def conta(self):
        Z_media = self.Z / self.m
        return math.floor(self.alpha * self.m * math.pow(2.0, Z_media))
\end{lstlisting}

A classe~\texttt{LogLog} do Programa~\ref{loglog:code} foi baseada no algoritmo~$\logprogram$. Vamos fazer alguns 
comentários sobre o método~\texttt{adiciona}. O primeiro deles é relacionado à identificação do lote de um elemento~$x$.
Nessa implementação, a variável \texttt{B} faz o papel da variável~$k$ do algoritmo~$\logprogram$. Vimos na 
Seção~\ref{sec:loglog:algorithm}, que os $k$ primeiros bits definem o lote. Logo, os \texttt{B} bits iniciais contém a
informação do lote de~$x$. Sabendo disto, precisamos de um modo de recortar esses bits. 

Um jeito de fazermos isto é montarmos um número tal que os primeiros \texttt{B} bits dele sejam~1 e os restantes, 0. 
Vamos chamá-lo de $y$. Nesse sentido, $x \mathbin{\&} y$ resultará em um inteiro $z$ cuja representação binária tem as 
seguintes propriedades: os \texttt{B} primeiros bits correspondem aos \texttt{B} bits iniciais de $x$, e os bits 
restantes são zero. Portanto, $z$ é justamente o lote de~$x$. Falta, assim, saber como calcular $y$. Um número com as 
mesmas características de $y$ é~$2^{\texttt{B}} - 1$, e podemos calculá-lo usando o operador $<<$, que é equivalente à 
potenciação na base $2$ quando aplicado ao número~$1$.

Um exemplo que pode deixar a ideia anterior clara é o seguinte: suponha que $\texttt{B} = 2$. Logo, 
$2^{\texttt{B}} - 1 = 2^2 - 1 = 3 = 11_2$. Note que todos os bits de $3$ estão ligados até a posição~$2$. Tomemos alguns
valores para $x$, como $7 = 111_2$ e $13 = 1011_2$. O lote do elemento $7$ é 
$7 \mathbin{\&} 3 = 111_2 \mathbin{\&} 110_2 = 110_2 = 3$, e de 13, 
$13 \mathbin{\&} 3 = 1011_2 \mathbin{\&} 1100_2 = 1000_2 = 1$. Podemos ver que os lotes calculados batem de fato com os 
valores esperados.

Uma vez calculado o lote de um elemento~$x$, precisamos remover os \texttt{B} bits iniciais dele para que possamos 
prosseguir com o algoritmo. Um modo de fazermos isto é utilizando o operador $>>$ que remove os bits menos 
significativos de um inteiro. Assim, seja $w = x >> \texttt{B}$. A representação binária de $w$ será a mesma que a de 
$x$, só que sem os primeiros \texttt{B} bits. Logo, tomando novamente $B = 2$ e $x = 13 = 1011_2$, $w$ seria igual a 
$1011_2 >> 2 = 11_2$.

Por fim, para que possamos ter um método \texttt{conta} com complexidade constante, precisamos ter a soma~$Z$ dos 
valores de $M$ já pré-computada para que a média deles seja calculada rapidamente. Dessa forma, a variável~$Z$ é mantida
atualizada a cada novo elemento inserido no método \texttt{adiciona}. 

\newpage
\section{Algoritmo $\HLOG$}
\label{sec:loglog:hyperloglog}

Apesar do algoritmo~$\LOG$ visto anteriormente ter um consumo de memória pelo menos $6$ menor que a $\pcounting$, o
desvio padrão dele, que é de $1{,}30 \mathbin{/} \sqrt{m}$, é maior. Isto pode ser um problema se tivermos que trabalhar 
com quantidades muito grandes, da ordem de bilhões. No entanto, com uma simples modificação no algoritmo~$\logprogram$, 
obtemos um novo algoritmo com o mesmo consumo de memória, mas desvio padrão de~$1{,}04 \mathbin{/} \sqrt{m}$.

O aprimoramento do algoritmo~$\LOG$ é conhecido como $\HLOG$~\citep{hyperloglog:07}. E a modificação que deve ser feita 
em~$\logprogram$ é substituir a média \textbf{aritmética} pela média \textbf{harmônica}. A intuição por trás desta 
substituição é que a média harmônica é afetada menos por \textit{outliers}, ou seja, valores muito fora do esperado não 
distorcem tanto a estimativa. A consequência disso é que a variância do estimador é reduzida. 

Tanto no algoritmo~$\LOG$ quanto no $\HLOG$, o valor de $M$ de cada lote é uma aproximação para $\lg n \mathbin{/} m$, e 
conseguimos obter uma estimativa para $n \mathbin{/} m$ se elevarmos~$2$ a esse valor. Dessa forma, no algoritmo~$\LOG$, 
a média aritmética das aproximações de $\lg n \mathbin{/} m$ é calculada. Por outro lado, no algoritmo~$\HLOG$, 
calculamos a média harmônica das aproximações de $n \mathbin{/} m$. Podemos encontrar esse média $\overline{M}$ da 
seguinte forma:

\[ \overline{M} = \bigg(\frac{\sum\big(2^{M[i]}\big)^{-1}}{m}\bigg)^{-1} = \frac{m}{\sum2^{-M[i]}} \]

O fato de trocarmos a média aritmética pela harmônica também interfere no fator de correção $\alpha_m$. Na prática, 
podemos usar a aproximação $\alpha_{\infty} \approx 0{,}7213$ assim que $m$ for maior que 128~\citep{HyperLogLogWiki}.
Por fim, o algoritmo~$\hlogprogram$ traduz essas ideias para pseudocódigo, e é possível notar que ele é praticamente 
idêntico ao algoritmo~$\logprogram$. A única diferença entre essas soluções está na última linha.

\begin{codebox}
      \Procname{$\hlogprogram(\Mbb, m = 2^k)$}
      \li \For $i$ de $0$ até $m$
            \Do
      \li   $M[i] \gets 0$
            \End
      \li \For cada $x$ em $\Mbb$ 
            \Do
      \li   $b_1b_2{\dots} \gets h(x)$
      \li   $lote \gets \langle b_1 {\dots} b_k \rangle$
      \li   $M[lote] \gets \max(M[lote], \rho_1(b_{k+1}b_{k+2}{\dots}))$
            \End
      \li
      \Return $\alpha_m \times m^2 \mathbin{/} \sum_i{2^{-M[i]}}$   
      \End
\end{codebox}

\newpage
\section{Implementando $\HLOG$}

\begin{lstlisting}[style=mypython,caption=Implementação do algoritmo $\hlogprogram$,captionpos=b, label=hloglog:code]
class HyperLogLog:
      def __init__(self, m=64):
          self.m = m
          self.B = math.floor(math.log2(m))
          self.M = [0] * m
          self.Z = self.m
          self.alpha = 0.7213
  
      def p(self, x: int):
          return (x & -x).bit_length()
  
      @property
      def prefixo(self):
          return (1 << self.B) - 1
  
      def adiciona(self, x: int):
          lote = x & self.prefixo
          w = x >> self.B
  
          if self.p(w) > self.M[lote]:
              self.Z -= math.pow(2, -self.M[lote])
              self.M[lote] = self.p(w)
              self.Z += math.pow(2, -self.M[lote])
  
      def conta(self):
          return math.floor(self.alpha * self.m * self.m / self.Z)
\end{lstlisting}

O Programa~\ref{hloglog:code} foi baseado no algoritmo~$\hlogprogram$ e é quase idêntico ao Programa~\ref{loglog:code}, 
que foi inspirado no algoritmo~$\logprogram$. Na implementação do $\LOG$, a variável~\texttt{Z} era a soma dos valores 
de \texttt{M}. Como no $\HLOG$, a média aritmética é substituída pela harmônica, a variável~\texttt{Z} passa a guardar 
outro tipo de soma. Em vista disso, o formato do estimador também muda, e consequentemente, o método~\texttt{conta} da 
classe \texttt{HyperLogLog} é ligeiramente diferente do mesmo método da classe \texttt{LogLog}.

Essa troca de médias também afeta o método~\texttt{adiciona}. As implementações apresentadas retornam a estimativa da 
quantidade de elementos distintos em tempo constante, e assim, a cada novo item inserido no método~\texttt{adiciona}, 
devemos atualizar a variável~\texttt{Z} para evitar computá-la toda vez que o método~\texttt{conta} for chamado. E é 
justamente essa atualização da variável~\texttt{Z} que difere nos dois programas.