\chapter{Contagem aproximada}
\label{chap:morris}


\section{O Problema}

O problema da \textbf{contagem aproximada} consiste em contar um grande número de elementos usando pouca memória.  
Esse problema foi abordado pela primeira vez por Robert  Morris no artigo ~\citep{morris:78}, 
em que o autor descreve a tentativa de se contar eventos cujas frequências podiam chegar até 130 mil, mas usando apenas contadores de 8 bits.

Um registrador de $n$ bits pode armazenar inteiros entre $0$ e $2^n-1$. Dessa forma, em um computador que possui registradores de 8 bits, pode-se contar até 255.
Assim, Morris projetou um algoritmo para armazenar contagens \textit{aproximadas}, uma vez que, não era possível manter as frequências \textit{exatas} dos eventos 
em sua máquina.


\section{Algoritmo de Morris}

Para se armazenar \textit{exatamente} um valor entre $0$ e $n$, são necessários registradores com $\Omega(\log n)$ bits. 
Então, para que seja possível contar o número de elementos em um conjunto com até $n$ elementos usando menos bits, 
deve-se abandonar a \textit{exatidão} da contagem e buscar alternativas aceitáveis.

Uma das primeiras ideias é manter o valor de $\log_2 n$ ao invés de $n$. 
Neste caso, se $X$ fosse o valor armazenado em um contador, $2^X$ seria utilizado como estimador \textit{aproximado} de $n$.
Para isto, seriam utilizados $O(\log \log n)$ bits.
Dessa forma, a política de incremento desse contador passa a ser a questão central.
Morris propôs um método probabilístico para resolver esse incremento.

Morris sugeriu originalmente utilizar um contador $X$ para armazenar a contagem do número de elementos em um dado conjunto $\mathbb{M}$.
A cada novo elemento examinado de $\mathbb{M}$, o contador $X$ seria incrementado com probabilidade $2^{-X}$.
Assim, o valor $2^{X} - 1$ passaria a ser adotado como a estimativa do total de elementos, 
sendo que o $-1$ é para que essa aproximação se ajustasse à situação em que não houvessem itens, ou seja, $M = \emptyset$.

A versão da estratégia de Morris que pode ser vista no Algoritmo \ref{prog:morris}
é baseada em uma ideia da seção \textit{Algorithm} do artigo ~\citetitle*{ApproximateCountingAlgorithm}~\citep{ApproximateCountingAlgorithm}.
Essa versão expressa a condição de incremento do contador como um experimento de lançamento de moedas. 
Sempre que um novo elemento do conjunto $\mathbb{M}$ é examinado, o lançamento de $X$ moedas é simulado. 
O contador é incrementado somente quando os resultados de todos os lançamentos forem cara.
E a probabilidade deste evento ocorrer é $2^{-X}$. Na prática, para mimetizar o jogar de moedas, 
um número aleatório $r$ de $X$ bits é gerado, e $X$ é incrementado apenas se $r = 0$.

Segue o pseudocódigo:
\begin{programruledcaption}{Algoritmo de Morris\label{prog:morris}}
  \begin{lstlisting}[
    language={[brazilian]pseudocode},
    style=pseudocode,
    style=wider,
    functions={},
    specialidentifiers={},
    deletekeywords={de}
  ]
      funcao Morris($\mathbb{M}$)  // \textbf{Recebe} conjunto $\mathbb{M}$ e \textbf{devolve} estimativa para $|\mathbb{M}|$
        X := 0
        para cada elemento de $\mathbb{M}$ \kw{faça}
          jogue uma moeda X vezes
          se X = 0 ou os resultados de todos os lançamentos forem cara \kw{faça}
            X := X + 1
          fim
        fim
        devolva $2^X - 1$
      fim
  \end{lstlisting}
\end{programruledcaption}

\section{Análise da aproximação}
\label{sec:morris:analysis}

Esta seção busca esclarecer o quanto a estimativa do Algoritmo \ref{prog:morris} se distância do tamanho real do conjunto $\mathbb{M}$ dado. As provas
serão baseadas nas notas de aula de \citep{LectureNotesAndoni}.
Considere que $X_n$ é a estimativa devolvida por este algoritmo tendo como entrada um conjunto $\mathbb{M}$ com $n$ elementos.

\begin{lemma} \label{morris:expected_value}
A estimativa do Algoritmo \ref{prog:morris} é não-viesada, ou seja, $\mathbb{E}[2^{X_n} - 1] = n$.
\end{lemma}

\begin{proof}
A prova será por indução em $n$. 

\textbf{Caso base:} Para $n = 0$, $X_n$ = 0. Logo, $\mathbb{E}[2^0 - 1] = 0$.

\textbf{Passo indutivo:} Suponha que $n > 0$ e que $\mathbb{E}[2^{X_{n-1}} - 1] = n-1$.

\begin{align*}
  \mathbb{E}[2^{X_n} - 1] 
    &= \sum_{x} \mathbb{P} (X_{n-1} = x) \mathbb{E}[2^{X_n} - 1 | X_{n-1} = x] \\
    &= \sum_{x} \mathbb{P} (X_{n-1} = x) \Big[ (2^{x+1} - 1) \frac{1}{2^x} +  (2^x - 1) (1 - \frac{1}{2^x}) \Big] \\
    &= \sum_{x} \mathbb{P} (X_{n-1} = x) 2^x \\
    &= \sum_{x} \mathbb{P} (X_{n-1} = x) (2^x - 1) + \sum_{x} \mathbb{P} (X_{n-1} = x) \\
    &= \mathbb{E}[2^{X_n-1} - 1] + 1 \\
    &= n - 1 + 1 \\
    &= n
\end{align*}

\end{proof}

\begin{lemma} \label{morris:variance}
$\mathbb{V}[2^{X_n} - 1] \leq \frac{3n(n+1)}{2} + 1$
\end{lemma}

\begin{proof}
Primeiro, aplica-se a definição de \hyperref[ap:variance]{variância} em $2^{X_n} - 1$, de modo que

\begin{align*}
  \mathbb{V}[2^{X_n} - 1] 
    &= \mathbb{E}[(2^{X_n} - 1) ^ 2] - \mathbb{E}[X_n]^2  \\
    &= \mathbb{E}[2^{2X_n} -2 \ 2^{X_n} + 1] - n^2 \\
    &= \mathbb{E}[2^{2X_n}] \underbrace{-2 \mathbb{E} [2^{X_n}] + 1 - n^2}_{\leq 0} \\ 
    &\leq \mathbb{E}[2^{2X_n}]
\end{align*}

Agora, deve-se calcular $\mathbb{E}[2^{2X_n}]$.

\begin{align*}
  \mathbb{E}[2^{2X_n}]  
    &=  \sum_{x} 2^{2x} \mathbb{P}(X_n = x) \\
    &=  \sum_{x} 2^{2x} \Big[ 2^{-(x-1)} \mathbb{P}(X_{n-1} = x-1) + (1 - 2^{-x}) \mathbb{P}(X_{n-1} = x) \Big] \\
    &=  \sum_{x} 2^{x+1} \mathbb{P}(X_{n-1} = x-1) + \sum_{x} 2^{2x} \mathbb{P}(X_{n-1} = x) - \sum_{x} 2^{x} \mathbb{P}(X_{n-1} = x) \\
    &=  \sum_{x} 4 \ 2^{x-1} \mathbb{P}(X_{n-1} = x-1) + \sum_{x} 2^{2x} \mathbb{P}(X_{n-1} = x) - \sum_{x} 2^{x} \mathbb{P}(X_{n-1} = x)  \\
    &=  4 \mathbb{E}[2^{X_{n-1}}] + \mathbb{E}[2^{2X_{n-1}}] - \mathbb{E}[2^{X_{n-1}}]  \\
    &=  3 \mathbb{E}[2^{X_{n-1}}] + \mathbb{E}[2^{2X_{n-1}}]  \\
    &=  3 n + \mathbb{E}[2^{2X_{n-1}}]  \\
\end{align*}

Note que $\mathbb{E}[2^{2X_n}]$ pode ser calculado recursivamente. Assim,

\begin{align*}
  \mathbb{E}[2^{2X_{n}}] 
    &= 3n + 3(n-1) + 3(n-2) + \dots + E[2^{2X_2}] \\
    &= 3n + 3(n-1) + 3(n-2) + \dots + 3 \ 2 + E[2^{2X_1}] \\
    &= 3n + 3(n-1) + 3(n-2) + \dots + 3 \ 2 + 3 \ 1 + E[2^{2X_0}] \\
    &= 3n + 3(n-1) + 3(n-2) + \dots + 3 \ 2 + 3 \ 1 + 1 \\
    &= \frac{3n(n+1)}{2} + 1
\end{align*}

Portanto, 
\[ \mathbb{V}[2^{X_n} - 1] \leq \mathbb{E}[2^{2X_n}] = \frac{3n(n+1)}{2} + 1 \]

\end{proof}


\begin{lemma}
  O Algoritmo \ref{prog:morris} consome $O(\log \log n)$ de memória com probabilidade acima de $90\%$.
\end{lemma}

\begin{proof}
  Usando a \nameref{ap:markov} com $X = 2^{X_n} - 1$ e $\alpha = 10n$, segue que

\[ \mathbb{P}(2^{X_n} - 1 \geq 10n)  \leq \frac{\mathbb{E}[2^X_n - 1]}{(10n)^2} = \frac{n}{100n^2} = \frac{1}{100n} \leq \frac{1}{10} \]

Assim, com probabilidade maior que $\frac{9}{10} = 90\%$, $2^{X_n} - 1 \leq 10n$. Logo, 

\[ 2^{X_n} - 1 \leq 10n  \]
\[ X_n \leq \log_2(10n + 1)\]

Para armazenar $X_n$, gasta-se $\log_2(X_n)$ bits. Assim,

\begin{align*}
  \log_2(X_n) 
    &\leq \log_2\log_2(10n + 1) \\ 
    &= O(\log \log n)
\end{align*} 

Portanto, o Algoritmo \ref{prog:morris} consome $O(\log \log n)$ de memória com probabilidade acima de $90\%$.

\end{proof}

Agora, pode-se usar os Lemas \ref{morris:expected_value} e \ref{morris:variance} para se conseguir uma estimativa para o 
erro do Algoritmo \ref{prog:morris}.
Usando a \nameref{ap:chebyshev} com $X = 2^{X_n} - 1$, segue que

\[ \mathbb{P}(|2^{X_n} - 1 - \mathbb{E}[2^{X_n} - 1]| \geq \sigma ) \leq \frac{Var[2^{X_n} - 1]}{\sigma^2}\]

\begin{align*}
  \mathbb{P}(|2^{X_n} - 1 - n| \geq \sigma ) 
    &\leq \frac{3\frac{n(n+1)}{2} + 1}{\sigma^2}  \\
    &= \frac{3n^2 + 3n + 2}{2\sigma^2}
\end{align*}


Tomando $\sigma = \epsilon n$, conclui-se

\begin{align*}
  \mathbb{P}(|2^{X_n} - 1 - n| \geq \epsilon n) 
    &\leq \frac{3n^2 + 3n + 2}{2 \epsilon^2 n^2}  \\
    &\leq \frac{4n^2}{2 \epsilon^2 n^2}  &&(\text{$3n + 2 \leq n^2$ para $n \geq 4$}) \\
    &= \frac{2}{\epsilon^2}
\end{align*}

Tome $\epsilon = \sqrt{40} \approx 6 $, de modo que 
\[ \mathbb{P}(|2^{X_n} - 1 - n| \geq \sqrt{40} n)  \leq 0.05 \]

Ou seja, com probabilidade de até $95\%$, o erro relativo cometido pelo Algoritmo \ref{prog:morris} pode ser de até $6$ vezes o valor de $n$.


\section{Melhorando a precisão do algoritmo}

Pela análise \ref{sec:morris:analysis} do Algoritmo \ref{prog:morris}, nota-se que esta solução apresenta uma grande variabilidade. 
Esta seção busca abordar uma técnica para diminuir este problema e, também, é baseada nas notas de aula de \citep{LectureNotesAndoni}.

A ideia principal é executar $k$ vezes o Algoritmo \ref{prog:morris} e a nova estimativa será a média aritmética dos resultados
de cada execução. Será visto que isso diminuirá a variância da estimativa.

Segue o pseudocódigo do algoritmo adaptado:
\begin{programruledcaption}{Algoritmo de Morris adaptado\label{prog:morris:plus}}
  \begin{lstlisting}[
    language={[brazilian]pseudocode},
    style=pseudocode,
    style=wider,
    functions={},
    specialidentifiers={},
  ]
      funcao Morris+(M, K)  // \textbf{Recebe} conjunto $\mathbb{M}$ e \textbf{devolve} estimativa para $|\mathbb{M}|$
        Y := 0
        para k de 1 até K \kw{faça}
          $X_k$ := Morris(M)
          Y := Y + $X_k$
        fim
        devolva $\frac{Y}{K}$
      fim
  \end{lstlisting}
\end{programruledcaption}

Considere que $Y_n$ é a saída do Algoritmo \ref{prog:morris:plus} para uma entrada de tamanho $n$.

\begin{lemma}\label{morris:plus:expected_value}
  $\mathbb{E}[Y_n] = n$
\end{lemma}

\begin{proof}

\begin{align*}
  \mathbb{E}[Y_n] 
    &= \mathbb{E} \Big[ \frac{1}{k} \sum_{i=1}^{k} X_n \Big]  \\
    &= \frac{1}{k} \sum_{i=1}^{k} \mathbb{E}[X_n] \\
    &= \frac{1}{k} k n  \\
    &= n
\end{align*}

\end{proof}

\begin{lemma}\label{morris:plus:variance}
  $\mathbb{V}[Y_n] \leq \frac{1}{k} ( \frac{3n(n+1)}{2} + 1 )$
\end{lemma}

\begin{proof}
  
\begin{align*}
  \mathbb{V}[Y_n] 
    &= \mathbb{V}[\frac{1}{k} \sum_{i=1}^{k} X_n] \\
    &= \frac{1}{k^2} \sum_{i=1}^{k}\mathbb{V}[X_n]  \\
    &\leq \frac{1}{k^2} \sum_{i=1}^{k} \Big( \frac{3n(n+1)}{2} + 1 \Big)  \\
    &= \frac{1}{k^2} k \Big( \frac{3n(n+1)}{2} + 1 \Big)  \\
    &= \frac{1}{k} \Big( \frac{3n(n+1)}{2} + 1 \Big)
\end{align*}

\end{proof}

Agora, pode-se usar os Lemas \ref{morris:plus:expected_value} e \ref{morris:plus:variance} para se conseguir uma estimativa para o 
erro do Algoritmo \ref{prog:morris:plus}. De forma análoga ao que foi feito na conclusão da Análise \ref{sec:morris:analysis}, pode-se 
concluir que

\[ \mathbb{P}(|Y_n - n| \geq \epsilon n ) \leq \frac{2}{\epsilon^2 k}\]

em que $\epsilon$ é o erro relativo do Algoritmo \ref{prog:morris:plus} e $k$, a quantidade de execuções do Algoritmo \ref{prog:morris}.
Assim, para um intervalo de confiança de $\delta$, pode-se escrever $k$ em função de $\epsilon$ e $\delta$:

\begin{align*}
      &\frac{2}{\epsilon^2 k^2} = 1 - \delta \\
  \iff& k = \Bigg\lceil \frac{2}{(1 - \delta) \epsilon^2} \Bigg\rceil
\end{align*}

Por fim, fixando um intervalo de confiança de $95\%$ e erro relativo de $10\%$ ($\epsilon = 0.1$), $k$ deve ser $4000$.  
