\chapter{Contagens aproximadas}
\label{chap:morris}


Na década de 1970, Robert Morris ~\citep{morris:78} estudou o problema de contar rapidamente \textit{trigramas} cujas 
frequências podiam chegar até 130 mil. Esses eventos eram trios de caracteres que ocorriam em textos.

O objetivo, então, era utilizar as contagens de trigramas na implementação de corretores ortográficos estátisticos 
~\citep{morris:lorinda:75} para editores de textos ~\citep{mcmahon:cherry:morris:78} que acompanhariam as distribuições 
do sistema operacional Unix dos laboratórios da Bell ~\citep{lumbroso:2018}. Contagens similares têm aplicação no 
projeto de compressores de textos estatísticos ~\citep{text:compression:1990}.

Na época, Morris utilizava um computador PDP11 da DEC com memória de 8 bits para realizar as contagens. Com 8 bits, 
somos capazes de representar ou \textit{contar} de 0 até 255. Em geral, uma memória de $n$ bits pode armazenar números 
inteiros entre 0 e $2^{n} - 1$.

Devido à limitação de espaço e ao desejo de eficiência, Morris projetou um algoritmo para realizar estimativas ou 
\textit{contagens aproximadas}, uma vez que, não era possível manter as frequências \textit{exatas} dos trigramas. Com 
seu algoritmo, Morris trouxe aleatorização e probabilidade para o centro da discussão. E para tornarmos essa discussão 
mais precisa, consideraremos o problema de determinar o número de elementos de um dado \textbf{conjunto}~$\Mbb$. 

\begin{quote}
  \textbf{Problema} \proc{ContagemAproximada($\Mbb$)}: dado um conjunto $\Mbb~=~\{ e_1, e_2, \dots, e_n \}$, encontrar 
  estimador $\hat{n}$ para o número $n$ de elementos em $\Mbb$.  
\end{quote}

Quando $\Mbb$ é conhecido de antemão, teremos um problema que é dito \textbf{estático} ou \textit{\textbf{offline}}. 
Já em problemas \textbf{dinâmicos} ou \textit{\textbf{online}}, receberemos uma sequência de operações sobre~$\Mbb$ que 
não são previamente conhecidas e devem ser realizadas uma após a outra. Entre essas operações, existirão as 
\textbf{consultas} (\textit{queries}) que podem ser, por exemplo, encontrar o número de elementos em~$\Mbb$. Teremos, 
também, as \textbf{atualizações} (\textit{updates}) que nesse capítulo, serão modificações de~$\Mbb$ somente por meio de 
inserções de elementos.

Nesse sentido, o problema estudado por Morris pode ser considerado \textbf{incremental} (não há remoções de elementos), 
e dessa forma, o conjunto $\Mbb$ pode ser visto como sendo um \textbf{fluxo} $x_1, x_2, \dots, x_t, \dots$ de elementos, 
e as consultas seriam ``Quantos elementos o conjunto $\Mbb$ possui \textbf{neste instante}~$?$`` ou 
``Quantos elementos já passaram por este fluxo``.

No entanto, estudaremos detalhadamente nesse texto, as versões estatícas dos problemas apresentados, uma vez que, as 
análises dos algoritmos que resolvem esses problemas se baseiam nessas versões. E veremos que obter a versão 
\textit{online} a partir da \textit{offline} é uma tarefa simples, e assim, as aplicações e simulações desses algoritmos 
serão baseadas em versões dinâmicas.

\section{Algoritmo de Morris}

Para armazenarmos \textit{exatamente} um valor entre $0$ e $n$, são necessários registradores com~$\Omega(\lg n)$ bits. 
Então, para que seja possível contar o número de elementos em um conjunto~$\Mbb$ com até $n$ elementos usando menos 
bits, devemos abandonar a \textit{exatidão} da contagem e buscar alternativas aceitáveis.

Morris sugeriu utilizar um contador $X$ para armazenar o valor de $\lg n$ ao invés de $n$. Neste caso, o estimador 
\textit{aproximado} $\hat{n}$ de $n$ seria~$2^{X} - 1$. Este ``-1`` na expressão faz com que essa aproximação se ajuste 
à situação em que $\Mbb = \emptyset$. E um contador como este pode ser armazenado em $O(\lg X) = O(\lg \lg n)$ bits.

A política de incremento de $X$ passa a ser a questão central do algoritmo. Morris propôs uma estratégia simples para 
realizar esse acréscimo: a cada novo elemento examinado de~$\Mbb$, o contador $X$ seria incrementado com probabilidade~ 
$2^{-X}$.

Essa estratégia pode ser vista no Algoritmo $\proc{Morris}$ cuja implementação é baseada em uma ideia da seção 
\textit{Algorithm} do artigo ~\citetitle*{ApproximateCountingAlgorithm}~\citep{ApproximateCountingAlgorithm}.Essa versão 
expressa a condição de incremento do contador como um experimento de lançamento de moedas. Sempre que um novo elemento 
do conjunto $\Mbb$ é examinado, o lançamento de $X$ moedas é simulado. O contador é incrementado somente quando os 
resultados de todos os lançamentos forem cara. E a probabilidade deste evento ocorrer é~$2^{-X}$.

O Algoritmo $\proc{Morris}$ a seguir recebe um conjunto~$\Mbb = \{ e_1, e_2, \dots, e_n \}$ e devolve um estimador~
$\hat{n}$ para $n$. Este estimador é da forma $2^{X} - 1$ em que $X$ é um número inteiro não negativo, e veremos mais 
adiante que o seu valor esperado é $n$, ou seja, $\Ebb(\hat{n}) = \Ebb(2^{X} - 1) = n$.

\begin{codebox}
  \Procname{$\proc{Morris}(\Mbb)$}
  \li $X \gets 0$
  \li \For cada $e$ em $\Mbb$ 
  \li \Do
      $r \gets \text{número de caras resultantes dos lançamentos de $X$ moedas}$ 
  \li   \If $r = 0$
  \li   \Do
          $X \gets X + 1$
        \End
      \End
  \li
  \Return $2^{X} - 1$   
  \End
\end{codebox}

Na prática, para mimetizar o lançamento de moedas, utilizamos uma função de hash~$h_k$ que recebe um elemento $e$ de 
$\Mbb$ e devolve um inteiro aleatório $r = h(e)$ entre 0 e $2^{k} - 1$. Este número $r$ pode ser visto como um inteiro 
de $k$ bit em que cada bits representa o resultado do lançamento de uma moeda: 0 corresponde à cara e 1, à coroa. 

\begin{codebox}
  \Procname{$\proc{Morris}(\Mbb)$}
  \li $X \gets 0$
  \li \For cada $e$ em $\Mbb$ 
  \li \Do
      $r \gets h_{X}(e)$ 
  \li   \If $r = 0$
  \li   \Do
          $X \gets X + 1$
        \End
      \End
  \li
  \Return $2^{X} - 1$   
  \End
\end{codebox}

O parâmetro $k$ da função de hash $h_{k}$ é geralmente implícito e depende da linguagem de programação que estamos 
usando para implementar esse algoritmo. Se escolhermos a linguagem $C{+}{+}$ para codificar o Algoritmo $\proc{Morris}$, 
o valor de $k$ poderia ser igual a 32, caso o hash fosse do tipo \textit{int}, ou 64, se o hash fosse um 
\textit{long long}. E nesses casos em que $k$ é implícito, ao invés de verificar se o valor do hash é zero para 
incrementar $X$, deveríamos conferir se os primeiros $X$ bits do hash são zeros.

\begin{codebox}
  \Procname{$\proc{Morris}(\Mbb)$}
  \li $X \gets 0$
  \li \For cada $e$ em $\Mbb$ 
  \li \Do
      $r \gets h(e)$ 
  \li   \If primeiros $X$ bits de $r$ são iguals a zero
  \li   \Do
          $X \gets X + 1$
        \End
      \End
  \li
  \Return $2^{X} - 1$   
  \End
\end{codebox}

\section{Qualidade da aproximação}
\label{sec:morris:analysis}

Esta seção busca esclarecer o quanto a estimativa do Algoritmo \ref{prog:morris} se distância do tamanho real do conjunto $\Mbb$ dado. As provas
serão baseadas nas notas de aula de \citep{LectureNotesAndoni}.

Considere que $2^{X_n} - 1$ é a estimativa devolvida pela função Morris($\Mbb$) tendo como entrada um conjunto $\Mbb$ com $n$ elementos.
Neste caso, $X_n$ é o valor do contador $X$ antes do término da execução da função.
Evidentemente, $X_n$ é uma variável aleatória que depende de $n$ e dos resultados dos lançamentos de moedas feitos durante a execução do algoritmo.
Dessa forma, $2^{X_n} - 1$ é também uma variável aleatória.

Agora, determinar $\Ebb[2^{X_n} - 1]$ passa a ser a questão central.

Para $n = 0$, $X_0 = 0$ e portanto,
\[ \Ebb[2^{X_0} - 1] = \Ebb[2^0 - 1] = E[0] = 0 \ .\]

Para $n = 1$, $X_1 = 1$ devido à condição $X = 0$ na linha $5$ da função \ref{prog:morris}. E assim, 
\[ \Ebb[2^{X_1} - 1] = \Ebb[2^1 - 1] = E[1] = 1 \ .\]

A determinação de $E[2^{X_3} - 1]$ está ilustrada na árvore da Figura \ref{fig:morris-tree}.
Nessa árvore, encontram-se os possíveis resultados do contador $X$ durantes as iterações da função de Morris.
A setas da cor verde indicam que o contador foi incrementado e as da cor vermelha, que o contador permaneceu com o mesmo valor.

\begin{figure}
  \centering
  \includegraphics[scale=0.50]{figuras/morris-tree.png}
	\caption{Árvore de decisão para quando $|\Mbb| = 3$ no Algoritmo \ref{prog:morris}}\label{fig:morris-tree}
\end{figure}

Segue que
\begin{align*}
  \Ebb[2^{X_3} - 1] 
    &= \frac{2}{8} \Big(2^1 - 1\Big) + \frac{2}{8} \Big(2^2 - 1\Big) + \frac{3}{8}\Big(2^2 - 1\Big) + \frac{1}{8} \Big(2^3 - 1\Big) \\
    &= \frac{2}{8} \Big(1\Big) + \frac{2}{8} \Big(3\Big) + \frac{3}{8} \Big(3\Big) + \frac{1}{8} \Big(7\Big) \\
    &= \frac{24}{8} \\
    &= 3 \ .
\end{align*}

Em todos os exemplos calculados, $\Ebb[2^{X_n} - 1] = n$. O Lema a seguir mostra que isso não é coincidência.


\begin{lemma} \label{morris:expected_value}
A estimativa do Algoritmo \ref{prog:morris} é não-viesada, ou seja, $\Ebb[2^{X_n} - 1] = n$.
\end{lemma}

\begin{proof}
A prova será por indução em $n$. 

Para $n = 0$, $X_n$ = 0. Logo, $\Ebb[2^0 - 1] = 0$.

Suponha que para $n > 0$, $\Ebb[2^{X_{n-1}} - 1] = n-1$.

\begin{align*}
  \Ebb[2^{X_n} - 1] 
    &= \sum_{x} \mathbb{P} (X_n = x) (2^x - 1) \\
    &= \sum_{x} \mathbb{P} (X_{n-1} = x) \Ebb[2^{X_n} - 1 | X_{n-1} = x] \\
    &= \sum_{x} \mathbb{P} (X_{n-1} = x) \Big[ (2^{x+1} - 1) \frac{1}{2^x} +  (2^x - 1) (1 - \frac{1}{2^x}) \Big] \\
    &= \sum_{x} \mathbb{P} (X_{n-1} = x) 2^x \\
    &= \sum_{x} \mathbb{P} (X_{n-1} = x) (2^x - 1) + \sum_{x} \mathbb{P} (X_{n-1} = x) \\
    &= \Ebb[2^{X_{n-1}} - 1] + 1 \\
    &= n - 1 + 1 \\
    &= n \ .
\end{align*}

\end{proof}

Somente calcular $\Ebb[2^{X_n} - 1]$ não é suficiente para saber se a aproximação devolvida pelo Algoritmo \ref{prog:morris} é razoável.
Assim, o objetivo agora, é estimar o valor de $\Vbb[2^{X_n} - 1]$ e entender como as saídas do algoritmo estão distribuídas. 
O próximo Lema mostra uma possível estimativa para essa variância.

\begin{lemma} \label{morris:variance}
$\Vbb[2^{X_n} - 1] \leq \frac{3n(n+1)}{2} + 1$.
\end{lemma}

\begin{proof}
Primeiro, aplica-se a definição de \hyperref[ap:variance]{variância} em $2^{X_n} - 1$, de modo que

\begin{align*}
  \Vbb[2^{X_n} - 1] 
    &= \Ebb[(2^{X_n} - 1) ^ 2] - \Ebb[2^{X_n} - 1]^2  \\
    &= \Ebb[2^{2X_n} -2 \ 2^{X_n} + 1] - n^2 \\
    &= \Ebb[2^{2X_n}] \underbrace{-2 \Ebb [2^{X_n}] + 1 - n^2}_{\leq 0} \\ 
    &\leq \Ebb[2^{2X_n}] \ .
\end{align*}

Agora, deve-se calcular $\Ebb[2^{2X_n}]$.

\begin{align*}
  \Ebb[2^{2X_n}]  
    &=  \sum_{x} 2^{2x} \mathbb{P}(X_n = x) \\
    &=  \sum_{x} 2^{2x} \Big[ 2^{-(x-1)} \mathbb{P}(X_{n-1} = x-1) + (1 - 2^{-x}) \mathbb{P}(X_{n-1} = x) \Big] \\
    &=  \sum_{x} 2^{x+1} \mathbb{P}(X_{n-1} = x-1) + \sum_{x} 2^{2x} \mathbb{P}(X_{n-1} = x) - \sum_{x} 2^{x} \mathbb{P}(X_{n-1} = x) \\
    &=  \sum_{x} 4 \ 2^{x-1} \mathbb{P}(X_{n-1} = x-1) + \sum_{x} 2^{2x} \mathbb{P}(X_{n-1} = x) - \sum_{x} 2^{x} \mathbb{P}(X_{n-1} = x)  \\
    &=  4 \Ebb[2^{X_{n-1}}] + \Ebb[2^{2X_{n-1}}] - \Ebb[2^{X_{n-1}}]  \\
    &=  3 \Ebb[2^{X_{n-1}}] + \Ebb[2^{2X_{n-1}}]  \\
    &=  3 n + \Ebb[2^{2X_{n-1}}] \ . \\
\end{align*}

Note que $\Ebb[2^{2X_n}]$ pode ser calculado recursivamente. Assim,

\begin{align*}
  \Ebb[2^{2X_{n}}] 
    &= 3n + 3(n-1) + 3(n-2) + \dots + E[2^{2X_2}] \\
    &= 3n + 3(n-1) + 3(n-2) + \dots + 3 (2) + E[2^{2X_1}] \\
    &= 3n + 3(n-1) + 3(n-2) + \dots + 3 (2) + 3 (1) + E[2^{2X_0}] \\
    &= 3n + 3(n-1) + 3(n-2) + \dots + 3 (2) + 3 (1) + 1 \\
    &= \frac{3n(n+1)}{2} + 1 \ .
\end{align*}

Portanto, 
\[ \Vbb[2^{X_n} - 1] \leq \Ebb[2^{2X_n}] = \frac{3n(n+1)}{2} + 1 \ .\]

\end{proof}

Por fim, note que o consumo de memória do Algoritmo \ref{prog:morris} é também uma variável aleatória, uma vez que,
$X_n$ é uma variável aleatória e o consumo de espaço da função depende deste valor. O seguinte Lema mostra que na maior 
parte dos casos, o programa consome em torno de $O(\lg\lg n)$ bits.


\begin{lemma}
  O Algoritmo \ref{prog:morris} consome $O(\lg \lg n)$ bits de memória com probabilidade acima de $90\%$.
\end{lemma}

\begin{proof}
  Usando a \nameref{ap:markov} com $X = 2^{X_n} - 1$ e $\alpha = 10n$, segue que

\[ \mathbb{P}(2^{X_n} - 1 \geq 10n)  \leq \frac{\Ebb[2^X_n - 1]}{(10n)^2} = \frac{n}{100n^2} = \frac{1}{100n} \leq \frac{1}{10} \ . \]

Assim, com probabilidade maior que $\frac{9}{10} = 90\%$, $2^{X_n} - 1 \leq 10n$. Logo, 

\[ 2^{X_n} - 1 \leq 10n  \]
\[ X_n \leq \lg(10n + 1) \ .\]

Para armazenar $X_n$, gasta-se $\lg(X_n)$ bits. Assim,

\begin{align*}
  \lg(X_n) 
    &\leq \lg\lg(10n + 1) \\ 
    &= O(\lg \lg n) \ .
\end{align*} 

Portanto, o Algoritmo \ref{prog:morris} consome $O(\lg \lg n)$ bits de memória com probabilidade acima de $90\%$.

\end{proof}

Agora, pode-se usar os Lemas \ref{morris:expected_value} e \ref{morris:variance} para se conseguir uma estimativa para o 
erro do Algoritmo \ref{prog:morris}.
Usando a \nameref{ap:chebyshev} com $X = 2^{X_n} - 1$, segue que

\[ \mathbb{P}(|2^{X_n} - 1 - \Ebb[2^{X_n} - 1]| \geq \sigma ) \leq \frac{Var[2^{X_n} - 1]}{\sigma^2} \ . \]

Dessa forma, 

\begin{align*}
  \mathbb{P}(|2^{X_n} - 1 - n| \geq \sigma ) 
    &\leq \frac{3\frac{n(n+1)}{2} + 1}{\sigma^2}  \\
    &= \frac{3n^2 + 3n + 2}{2\sigma^2} \ .
\end{align*}


Tomando $\sigma = \epsilon n$, conclui-se

\begin{align*}
  \mathbb{P}(|2^{X_n} - 1 - n| \geq \epsilon n) 
    &\leq \frac{3n^2 + 3n + 2}{2 \epsilon^2 n^2}  \\
    &\leq \frac{4n^2}{2 \epsilon^2 n^2}  &&(\text{$3n + 2 \leq n^2$ para $n \geq 4$}) \\
    &= \frac{2}{\epsilon^2} \ .
\end{align*}

Tome $\epsilon = \sqrt{40} \approx 6 $, de modo que 
\[ \mathbb{P}(|2^{X_n} - 1 - n| \geq \sqrt{40} n)  \leq 0.05 \ . \]

Ou seja, com probabilidade de até $95\%$, o erro relativo cometido pelo Algoritmo \ref{prog:morris} pode ser de até $6$ vezes o valor de $n$.


\section{Melhorando a precisão do algoritmo}
\label{sec:morris:plus}

Pela análise \ref{sec:morris:analysis} do Algoritmo \ref{prog:morris}, nota-se que esta solução apresenta uma grande variabilidade. 
Esta seção busca abordar uma técnica para diminuir este problema e, também, é baseada nas notas de aula de \citep{LectureNotesAndoni}.

A ideia principal é executar $k$ vezes o Algoritmo \ref{prog:morris} e a nova estimativa será a média aritmética dos resultados
de cada execução. Será visto que isso diminuirá a variância da estimativa.

Segue o pseudocódigo do algoritmo adaptado:
\begin{programruledcaption}{
Algoritmo de Morris adaptado\label{prog:morris:plus}
\\ \textbf{Entrada:} conjunto $\Mbb$, quantidade $K$ de iterações do Algoritmo \ref{prog:morris} 
\\ \textbf{Saída:} estimativa de $|\Mbb|$
\label{prog:flajolet-martin}
}
  \begin{lstlisting}[
    language={[brazilian]pseudocode},
    style=pseudocode,
    style=wider,
    functions={},
    specialidentifiers={},
    deletekeywords={de}
  ]
      função Morris++($\Mbb$, $K$)
        Y := 0
        para k de 1 até K \kw{faça}
          $\tilde{X}_k$ := Morris($\Mbb$)
          Y := Y + $\tilde{X}_k$
        fim
        devolva $\frac{Y}{K}$
      fim
  \end{lstlisting}
\end{programruledcaption}

Considere que $Y_n$ é a saída do Algoritmo \ref{prog:morris:plus} para uma entrada de tamanho $n$.

\begin{lemma}\label{morris:plus:expected_value}
  $\Ebb[Y_n] = n$.
\end{lemma}

\begin{proof}

\begin{align*}
  \Ebb[Y_n] 
    &= \Ebb \Big[ \frac{1}{k} \sum_{i=1}^{k} X_n \Big]  \\
    &= \frac{1}{k} \sum_{i=1}^{k} \Ebb[X_n] \\
    &= \frac{1}{k} k n  \\
    &= n \ .
\end{align*}

\end{proof}

\begin{lemma}\label{morris:plus:variance}
  $\Vbb[Y_n] \leq \frac{1}{k} ( \frac{3n(n+1)}{2} + 1 )$.
\end{lemma}

\begin{proof}
  
\begin{align*}
  \Vbb[Y_n] 
    &= \Vbb[\frac{1}{k} \sum_{i=1}^{k} X_n] \\
    &= \frac{1}{k^2} \sum_{i=1}^{k}\Vbb[X_n]  \\
    &\leq \frac{1}{k^2} \sum_{i=1}^{k} \Big( \frac{3n(n+1)}{2} + 1 \Big)  \\
    &= \frac{1}{k^2} k \Big( \frac{3n(n+1)}{2} + 1 \Big)  \\
    &= \frac{1}{k} \Big( \frac{3n(n+1)}{2} + 1 \Big) \ .
\end{align*}

\end{proof}

Agora, pode-se usar os Lemas \ref{morris:plus:expected_value} e \ref{morris:plus:variance} para se conseguir uma estimativa para o 
erro do Algoritmo \ref{prog:morris:plus}. De forma análoga ao que foi feito na conclusão da Análise \ref{sec:morris:analysis}, pode-se 
concluir que

\[ \mathbb{P}(|Y_n - n| \geq \epsilon n ) \leq \frac{2}{\epsilon^2 k} \ . \]

em que $\epsilon$ é o erro relativo do Algoritmo \ref{prog:morris:plus} e $k$, a quantidade de execuções do Algoritmo \ref{prog:morris}.
Assim, para um intervalo de confiança de $\delta$, pode-se escrever $k$ em função de $\epsilon$ e $\delta$:

\begin{align*}
      &\frac{2}{\epsilon^2 k^2} = 1 - \delta \\
  \iff& k = \Bigg\lceil \frac{2}{(1 - \delta) \epsilon^2} \Bigg\rceil \ .
\end{align*}

Por fim, fixando um intervalo de confiança de $95\%$ e erro relativo de $10\%$ ($\epsilon = 0.1$), $k$ deve ser $4000$.  
